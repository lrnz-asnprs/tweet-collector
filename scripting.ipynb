{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regex for special characters\n",
    "from random import random\n",
    "import os, re, pandas as pd\n",
    "from datetime import datetime\n",
    "from fake_collector.configs.directory_config import Directories\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_excluded_words(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    tweet_tokenizer = TweetTokenizer()\n",
    "    word_tokens = tweet_tokenizer.tokenize(text)\n",
    "    #stopword removal\n",
    "    word_tokens = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    #special characters\n",
    "    word_tokens = [w for w in word_tokens if not re.match(r\"[^A-Za-z0-9]\", w)]\n",
    "    filtered_sentence = []\n",
    "    \n",
    "    return \" \".join(word_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "remove_excluded_words(sample.claim[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_character_limit(character_limit, text):\n",
    "    tweet_tokenizer = TweetTokenizer()\n",
    "    word_tokens = tweet_tokenizer.tokenize(text) \n",
    "    \n",
    "    sentence = [\" \".join([w]) for w in word_tokens if len(sentence)<character_limit]\n",
    "    \n",
    "    return sentence\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_claims(sample):\n",
    "    # #Cleaning step 1 - getting rid of \\n appearances and weird spaces\n",
    "    # sample['claim'] = sample.claim.apply(lambda x: x.strip())\n",
    "\n",
    "    # #Cleaning from all special characters - note the \\w'\\w is to avoid removing the ' in don't, won't they've etc.\n",
    "    # sample['claim'] = sample.claim.apply(lambda x: re.sub(r'[^a-zA-Z0-9 \\w\\'\\w \\.,]', '', x))\n",
    "\n",
    "    # Removing \"Says\" in the beginning of a statement around 1000 claim quotes in the sample have this sentence structure.\n",
    "    # Fine to remove as the quote after is the essence of the claim.\n",
    "    sample['claim'] = sample.claim.apply(lambda x: re.sub(r'^Says', '', x).strip())\n",
    "    \n",
    "    # remove stopwords - done in order to avoid query errors such as:\n",
    "    # \"There were errors processing your request: Ambiguous use of and as a keyword. Use a space to logically join two clauses, or \\\"and\\\" to find occurrences of and in text\n",
    "    sample['claim'] = sample.claim.apply(lambda x: remove_excluded_words(x))\n",
    "    \n",
    "\n",
    "def reshape_date_format(sample):\n",
    "    sample['date'] = sample.date.apply(lambda x: str(datetime.strptime(x, '%B %d, %Y').isoformat()+\"Z\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "#Cleaning and preparation\n",
    "clean_claims(sample)\n",
    "reshape_date_format(sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample.claim[test][0:140]\n",
    "\n",
    "sample.claim[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = 'November 25, 2018'\n",
    "datet = str(datetime.strptime(date, '%B %d, %Y').isoformat()+\"Z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_str = '09-19-2018'\n",
    "\n",
    "date_object = datetime.strptime(date_str, '%m-%d-%Y')[0:-4].date()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(datetime.today().isoformat()[0:-4]+\"Z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = Directories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(str(dir.DATA_PATH)+\"/fakenews_sources/politifact_scrape_7t_03102022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.groupby(df.origin).count()\n",
    "\n",
    "#df[df.origin=='Instagram posts'].count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv(str(dir.DATA_PATH)+\"/fakenews_sources/sample_tweets_for_run0310.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.groupby(df.origin).count().claim.index\n",
    "y = df.groupby(df.origin).count().claim.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view = df.groupby(df.origin).count().claim.to_frame()\n",
    "\n",
    "view.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view.sort_values(by='claim')\n",
    "len(view.index.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fake_collector.utils.read_lst_jsons import read_lst_of_json_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = read_lst_of_json_objects(\"data/fakenews_tw_output/00001_5_environment_false_2022-07-21.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets[0]['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gugy/Documents/tweet-collector/data datapath\n"
     ]
    }
   ],
   "source": [
    "from fake_collector.modules.FakeNewsTweetCollector import FakeNewsTweetCollector\n",
    "#regex for special characters\n",
    "from random import random\n",
    "import os, re, pandas as pd\n",
    "from datetime import datetime\n",
    "from fake_collector.configs.directory_config import Directories\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = Directories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv(str(dir.DATA_PATH)+\"/fakenews_sources/sample_tweets_for_run0310.csv\")\n",
    "\n",
    "fn = FakeNewsTweetCollector(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>origin</th>\n",
       "      <th>URL</th>\n",
       "      <th>truth_value</th>\n",
       "      <th>stated_on</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009 saved ratepayers around 500 million persu...</td>\n",
       "      <td>Josh Mandel</td>\n",
       "      <td>https://www.politifact.com/factchecks/2011/oct...</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>2011-10-03T00:00:00Z</td>\n",
       "      <td>environment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fish tank additive may treat coronavirus</td>\n",
       "      <td>John Cornyn</td>\n",
       "      <td>https://www.politifact.com/factchecks/2020/mar...</td>\n",
       "      <td>false</td>\n",
       "      <td>2020-03-20T00:00:00Z</td>\n",
       "      <td>coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>122 vicious prisoners released Obama Administr...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>https://www.politifact.com/factchecks/2017/mar...</td>\n",
       "      <td>barely-true</td>\n",
       "      <td>2017-03-07T00:00:00Z</td>\n",
       "      <td>terrorism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>money dedicated within Department Homeland Sec...</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>https://www.politifact.com/factchecks/2015/may...</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>2015-05-10T00:00:00Z</td>\n",
       "      <td>climate-change</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Smartmatic Dominion systems built thing changi...</td>\n",
       "      <td>Ron Johnson</td>\n",
       "      <td>https://www.politifact.com/factchecks/2020/nov...</td>\n",
       "      <td>pants-fire</td>\n",
       "      <td>2020-11-16T00:00:00Z</td>\n",
       "      <td>elections</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Plankton Atlantic Ocean 90 gone</td>\n",
       "      <td>Facebook posts</td>\n",
       "      <td>https://www.politifact.com/factchecks/2022/jul...</td>\n",
       "      <td>false</td>\n",
       "      <td>2022-07-21T00:00:00Z</td>\n",
       "      <td>environment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Republican governor 2014 elections took higher...</td>\n",
       "      <td>National Republican Senatorial Committee</td>\n",
       "      <td>https://www.politifact.com/factchecks/2015/mar...</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>2015-02-26T00:00:00Z</td>\n",
       "      <td>elections</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>weeks ago Obama took Hamas Iran terror list</td>\n",
       "      <td>Mark Pocan</td>\n",
       "      <td>https://www.politifact.com/factchecks/2015/mar...</td>\n",
       "      <td>false</td>\n",
       "      <td>2015-03-19T00:00:00Z</td>\n",
       "      <td>terrorism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Green New Deal would require 99 homes country ...</td>\n",
       "      <td>Facebook posts</td>\n",
       "      <td>https://www.politifact.com/factchecks/2020/nov...</td>\n",
       "      <td>false</td>\n",
       "      <td>2020-10-28T00:00:00Z</td>\n",
       "      <td>climate-change</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Twenty-six men named William elected statewide...</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>https://www.politifact.com/factchecks/2015/sep...</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>2015-09-22T00:00:00Z</td>\n",
       "      <td>elections</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               claim  \\\n",
       "0  2009 saved ratepayers around 500 million persu...   \n",
       "1           Fish tank additive may treat coronavirus   \n",
       "2  122 vicious prisoners released Obama Administr...   \n",
       "3  money dedicated within Department Homeland Sec...   \n",
       "4  Smartmatic Dominion systems built thing changi...   \n",
       "5                    Plankton Atlantic Ocean 90 gone   \n",
       "6  Republican governor 2014 elections took higher...   \n",
       "7        weeks ago Obama took Hamas Iran terror list   \n",
       "8  Green New Deal would require 99 homes country ...   \n",
       "9  Twenty-six men named William elected statewide...   \n",
       "\n",
       "                                     origin  \\\n",
       "0                               Josh Mandel   \n",
       "1                               John Cornyn   \n",
       "2                              Donald Trump   \n",
       "3                           Hillary Clinton   \n",
       "4                               Ron Johnson   \n",
       "5                            Facebook posts   \n",
       "6  National Republican Senatorial Committee   \n",
       "7                                Mark Pocan   \n",
       "8                            Facebook posts   \n",
       "9                           Hillary Clinton   \n",
       "\n",
       "                                                 URL  truth_value  \\\n",
       "0  https://www.politifact.com/factchecks/2011/oct...  mostly-true   \n",
       "1  https://www.politifact.com/factchecks/2020/mar...        false   \n",
       "2  https://www.politifact.com/factchecks/2017/mar...  barely-true   \n",
       "3  https://www.politifact.com/factchecks/2015/may...  mostly-true   \n",
       "4  https://www.politifact.com/factchecks/2020/nov...   pants-fire   \n",
       "5  https://www.politifact.com/factchecks/2022/jul...        false   \n",
       "6  https://www.politifact.com/factchecks/2015/mar...  mostly-true   \n",
       "7  https://www.politifact.com/factchecks/2015/mar...        false   \n",
       "8  https://www.politifact.com/factchecks/2020/nov...        false   \n",
       "9  https://www.politifact.com/factchecks/2015/sep...  mostly-true   \n",
       "\n",
       "              stated_on           topic  \n",
       "0  2011-10-03T00:00:00Z     environment  \n",
       "1  2020-03-20T00:00:00Z     coronavirus  \n",
       "2  2017-03-07T00:00:00Z       terrorism  \n",
       "3  2015-05-10T00:00:00Z  climate-change  \n",
       "4  2020-11-16T00:00:00Z       elections  \n",
       "5  2022-07-21T00:00:00Z     environment  \n",
       "6  2015-02-26T00:00:00Z       elections  \n",
       "7  2015-03-19T00:00:00Z       terrorism  \n",
       "8  2020-10-28T00:00:00Z  climate-change  \n",
       "9  2015-09-22T00:00:00Z       elections  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn.preprocess_data()\n",
    "fn.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011-10-03T00:00:00Z\n",
      "2022-10-04T00:00:00Z\n",
      "2009 saved ratepayers around 500 million persuading Council pursue less expensive compliance mechanism City required treat Bull Run drinking water\n",
      "2020-03-20T00:00:00Z\n",
      "2022-10-04T00:00:00Z\n",
      "Fish tank additive may treat coronavirus\n",
      "2017-03-07T00:00:00Z\n",
      "2022-10-04T00:00:00Z\n",
      "122 vicious prisoners released Obama Administration Gitmo returned battlefield\n",
      "2015-05-10T00:00:00Z\n",
      "2022-10-04T00:00:00Z\n",
      "money dedicated within Department Homeland Security climate change what's spent combating Islamist terrorists radicalizing Internet United States America\n",
      "2020-11-16T00:00:00Z\n",
      "2022-10-04T00:00:00Z\n",
      "Smartmatic Dominion systems built thing changing results elections\n",
      "2022-07-21T00:00:00Z\n",
      "2022-10-04T00:00:00Z\n",
      "Plankton Atlantic Ocean 90 gone\n",
      "2015-02-26T00:00:00Z\n",
      "2022-10-04T00:00:00Z\n",
      "Republican governor 2014 elections took higher percentage Republican vote yet also carried independents 12 points\n",
      "2015-03-19T00:00:00Z\n",
      "2022-10-04T00:00:00Z\n",
      "weeks ago Obama took Hamas Iran terror list\n",
      "2020-10-28T00:00:00Z\n",
      "2022-10-04T00:00:00Z\n",
      "Green New Deal would require 99 homes country torn rebuilt energy efficient ten year period\n",
      "2015-09-22T00:00:00Z\n",
      "2022-10-04T00:00:00Z\n",
      "Twenty-six men named William elected statewide office seven women honor\n"
     ]
    }
   ],
   "source": [
    "fn.get_fakenews_tweets(disable_fetch=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "36e2581dfb10b7e3de3547be529c2e8e9e4d59798f578734b808d68ebdb931d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
